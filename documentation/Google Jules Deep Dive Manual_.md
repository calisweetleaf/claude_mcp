# **JULES: The Asynchronous Coding Agent \- A Comprehensive Technical Manual**

## **Part 1: Introduction to JULES \- The Autonomous Coding Agent**

### **1.1 What is Google JULES? (Core Concept, Asynchronous Nature, Agentic Capabilities)**

Google JULES represents a significant advancement in AI-assisted software development, functioning as an **autonomous and agentic coding assistant**.1 Its core design revolves around **asynchronous operation**, a paradigm that allows JULES to work independently in the background on assigned coding tasks. This means developers can delegate complex or time-consuming activities to JULES and continue with other critical work, receiving notifications upon task completion rather than actively waiting.2 This approach fundamentally aims to enhance developer focus and streamline workflows by minimizing context switching.4  
JULES integrates directly with existing **GitHub repositories**.1 When a task is initiated, JULES performs a full clone of the specified codebase into a secure Google Cloud virtual machine (VM). This comprehensive cloning is a cornerstone of its capability, providing JULES with the **complete context of the repository**. Such deep contextual understanding allows it to analyze interdependencies, understand the broader project architecture, and execute complex, multi-file changes with greater accuracy.1  
The agent is engineered to handle a diverse array of practical software engineering tasks. These include, but are not limited to, **fixing bugs, refactoring code, upgrading dependencies, generating comprehensive changelogs (including innovative audio summaries of code changes), and even undertaking the initial development work for new features**.1 This range of capabilities positions JULES beyond simple code completion utilities, establishing it as a proactive problem-solving partner in the development lifecycle.3  
The emphasis on JULES's asynchronous and autonomous nature across numerous descriptions 2 points to a deliberate design choice aimed at redefining the developer's role. Rather than being directly involved in the generation of every line of code, as is common with many copilot-style tools, developers can delegate entire segments of work. This facilitates a model where developers manage a portfolio of tasks, with some being efficiently handled by AI agents like JULES. Such parallelization of effort can lead to substantial productivity increases, particularly for addressing backlog items, performing routine maintenance, or accelerating feature development.  
The term "agentic," frequently used to describe JULES 1, signifies more than mere automation. It implies a system possessing a degree of understanding, sophisticated planning capabilities, and the ability to autonomously pursue defined goals. Descriptions of an "agentic planning system" 2 and an "agent loop" involving code generation, execution, result analysis, and adjustment 2 substantiate this. This "agentic leap" suggests JULES is designed to comprehend developer intent and work towards an objective, rather than solely responding to explicit, low-level commands. This advanced capability opens the possibility of delegating more complex and less precisely defined tasks, allowing JULES to contribute more meaningfully to the software engineering process.

### **1.2 The Power Behind JULES: Understanding the Gemini 2.5 Pro Integration**

The sophisticated capabilities of JULES are fundamentally driven by **Google's Gemini 2.5 Pro model**.1 This model is characterized as Google's **state-of-the-art multimodal language model**, engineered to deliver superior performance in code understanding, generation, and complex reasoning tasks when compared to its predecessors.2  
It is this powerful AI engine that endows JULES with the ability to perform **advanced reasoning across entire codebases**. Gemini 2.5 Pro enables JULES to understand intricate relationships between different files and modules, and to plan and execute complicated, multi-file changes that require a holistic view of the project.6  
The consistent highlighting of "advanced code understanding and reasoning abilities" 2 indicates that JULES's core strength extends beyond simple code generation. Its primary value lies in its capacity to comprehend the semantics, intent, and broader architectural context of a software project. Tasks such as "refactoring logic" 1 or "building out a new feature" 5 inherently demand this deeper level of understanding, which Gemini 2.5 Pro is specifically designed to provide. This suggests JULES is capable of tackling problems that are more abstract or require nuanced interpretation than those manageable by simpler code generation tools.  
Furthermore, by architecting JULES upon a flagship multimodal model like Gemini 2.5 Pro, Google has established a platform poised for organic improvement as the underlying AI model evolves. As future iterations of Gemini models gain enhanced capabilities—such as larger context windows, more refined reasoning algorithms, improved handling of diverse data types, or even new modalities relevant to software development (e.g., understanding UI mockups)—JULES will naturally inherit these advancements. This positions JULES not as a static product with a fixed set of features, but as a dynamic tool with a significant and continuous growth trajectory. The ongoing integration of Gemini into other developer tools, such as Android Studio for features like "Journeys" (automated app testing) and automated dependency upgrades 8, exemplifies this commitment to continuous evolution and enhancement of AI-assisted development capabilities.

### **1.3 Core Philosophy: Developer in the Loop and Asynchronous Productivity**

A central tenet in the design philosophy of JULES is the principle of keeping the **developer "firmly in control"**.2 JULES is not envisioned as a fully autonomous system that operates without human oversight or intervention. Instead, it functions as a powerful assistant that requires human guidance and approval at critical junctures.  
Before any code is executed or modifications are applied to the codebase, JULES **generates a detailed, multi-step plan**. This plan clearly articulates the sequence of actions JULES intends to perform to address the developer's request.1 The plan is presented to the developer for **thorough review and requires explicit approval** before JULES proceeds.1 This transparency is a key feature, allowing developers to understand JULES's proposed approach and reasoning.  
Developers are empowered to **interact with JULES, and to modify or refine the proposed plan** if the initial proposal is not perfectly aligned with their expectations or project constraints. This interaction often occurs through an interactive chat interface, enabling a dialogue between the developer and the AI agent.2  
Once JULES has completed its work based on the approved (and potentially refined) plan, the resulting changes are typically packaged as a **GitHub pull request (PR)**. This allows for a final, comprehensive human review of the generated code within the familiar Git workflow before any changes are merged into the project's main development branches.1  
The entire workflow, from the initial plan proposal to the final pull request, is meticulously designed to foster developer trust and confidence. By transparently presenting its intended actions and the reasoning behind them ("what it intends to do and why" 2), JULES demystifies its internal processes. The multiple checkpoints for human intervention—plan review, chat-based refinement during planning or execution 2, and final PR approval—ensure that developers do not feel they are relinquishing control over their codebase. This iterative, controllable process is paramount for the successful adoption of powerful AI tools like JULES in critical software development environments.  
JULES is consistently framed as a tool designed to handle "grunt work" 2, "random tasks that you'd rather not" 5, or to function as an "autonomous developer intern".1 This strategic delegation allows human developers to concentrate their efforts on "more complex tasks" 2 or "the coding you want to do".5 The overarching aim is clearly one of augmentation and productivity multiplication, rather than developer replacement. This framing is crucial for addressing potential anxieties within the development community and for encouraging a collaborative human-AI teaming approach to software development. The explicit statement that JULES "Can Assist – But Does Not Replace – Developers" 9 further underscores this collaborative vision.  
The following table provides a synthesized overview of JULES's primary functionalities:  
**Table 1: JULES Core Capabilities Overview**

| Capability | Description | Key Benefit to Developer | Example Use Case from Sources |
| :---- | :---- | :---- | :---- |
| Bug Fixing | Automatically identifies and proposes patches for bugs within the codebase. | Reduces time spent on debugging and resolving common errors. | "Fixing bugs" 1, "Resolve the null pointer error in the auth.js file".10 |
| Dependency Upgrades | Manages and executes updates for project dependencies, addressing potential compatibility issues. | Keeps projects current and secure with minimal manual effort. | "Bumping dependencies" 1, "Upgrade to Node.js 18 and fix compatibility issues".3 |
| New Feature Scaffolding | Can take an initial pass at building out new features based on developer prompts or specifications. | Accelerates the initial stages of feature development. | "Take the first cut at building out a new feature" 5, "Building new features".1 |
| Test Generation & Updates | Capable of writing new unit tests and updating existing ones to reflect code changes. | Improves code quality and test coverage, ensuring robustness. | "Writing and updating unit tests" 6, "Create unit tests for the mathUtils.js module".10 |
| Code Refactoring | Intelligently refactors existing code to improve structure, readability, or performance. | Enhances maintainability and efficiency of the codebase. | "Refactoring code" 1, "Optimize a nested loop in a sorting algorithm".10 |
| Changelog Generation | Creates textual summaries of code changes. | Facilitates communication and documentation of project evolution. | "Generating changelogs".1 |
| Audio Changelogs | Converts code changes and commit summaries into audible narratives. | Offers a novel way for teams to stay updated, e.g., during commutes or async syncs. | "Audio changelogs" 3, "Sprint recaps... auditory brief".1 |
| Multi-Step Task Execution | Breaks down complex requests into a sequence of manageable steps and executes them. | Handles complex, multi-faceted tasks that go beyond simple code completion. | "Multi-step task execution" 3, "Jules proposes a structured plan".1 |
| Full Repository Context | Clones and analyzes the entire repository to understand the full project context before making changes. | Enables more accurate and contextually relevant code modifications across multiple files. | "Understands the full context of your repository" 1, "Full-repo context".7 |
| Asynchronous Operation | Works in the background, allowing developers to continue with other tasks. | Maximizes developer productivity by eliminating waiting times. | "Asynchronous coding agent" 2, "Works quietly in the background".6 |
| GitHub Integration & PRs | Integrates directly with GitHub, creating pull requests for review and merging. | Streamlines the code review and integration process using existing developer workflows. | "Creates a PR that you can merge" 5, "Full PR generation with diff comparison".3 |

## **Part 2: The JULES Virtual Environment (VM) \- Under the Hood**

### **2.1 Genesis of a Task: How JULES Clones Your Repository into a Secure Cloud VM**

The operational lifecycle of a JULES task commences when a developer assigns a specific coding objective. Upon receiving this assignment, the JULES system provisions a **dedicated, fresh virtual machine (VM) instance within the secure infrastructure of Google Cloud**.2 This practice of instantiating a new VM for each task is fundamental to ensuring a clean, sterile, and isolated environment, preventing any potential interference from previous operations or other concurrent tasks.  
Following the VM provisioning, JULES proceeds to **clone the entirety of the specified GitHub repository, including the designated branch**, into this newly created virtual machine.1 This full-repository cloning strategy is a critical differentiator, as it provides JULES with comprehensive access to all code, configuration files, documentation, and historical context embedded within the project.  
This VM then serves as a **secure sandbox environment**.2 Within this contained space, JULES can safely perform a multitude of operations: it can analyze the structure and content of the code, install necessary dependencies (e.g., libraries, frameworks), execute build processes, run automated test suites, and make the code modifications required to fulfill the assigned task. All these actions occur in isolation, without any direct impact on the developer's local environment or the original GitHub repository, until a formal pull request is generated and subsequently approved by the developer.  
A key aspect of this architecture is the ephemeral nature of these task-specific VMs. **Each task operates within its own isolated VM, and all data contained within that VM—including the cloned codebase, installed dependencies, and any intermediate files generated during processing—is temporary and securely deleted after the task is completed** and its results are delivered (typically as a pull request).3  
The consistent emphasis on terms such as "secure Google Cloud virtual machine" 1, "sandbox" 2, "isolated VMs" 3, and the explicit statement that data is "temporary and deleted after use" 3 underscores a robust and deliberate security architecture. This level of isolation is paramount for protecting intellectual property, preventing unintended side effects from experimental code changes, and building developer trust, particularly in enterprise settings. The assurance that "no private data is used for AI training" on code from private repositories 3 further solidifies this commitment to data privacy and security.  
Furthermore, the VM-per-task model 2 inherently provides the architectural underpinnings for scalability and concurrency. Because each task is self-contained within its own dedicated VM, multiple tasks can, in principle, be executed in parallel without concerns about resource contention or cross-task interference. This architectural choice directly supports JULES's advertised capability for "concurrent task handling" 6, with the primary limitations on parallelism being platform-imposed usage quotas rather than inherent architectural bottlenecks. This design facilitates a more efficient use of developer time, as multiple coding chores can be delegated and processed simultaneously.

### **2.2 Anatomy of the JULES VM**

Understanding the composition of the JULES Virtual Machine provides deeper insight into its operational capabilities and the environment in which it executes tasks.

#### **2.2.1 Operating Environment and Sandboxing**

The JULES VM is explicitly described as a **"full Linux environment"**.2 This choice of operating system provides a standardized, powerful, and widely understood foundation for its operations. Linux offers a rich ecosystem of tools and utilities that can be leveraged by JULES and by the setup scripts developers provide.  
It is characterized as a **"secure, disposable Google Cloud virtual machine"**.11 The term "disposable" reinforces its ephemeral nature: each VM is created for a single task and then decommissioned. This ensures that each task begins with a pristine environment, free from any state or artifacts from previous tasks.  
All operations conducted by JULES within this VM are **"sandboxed"**.2 Sandboxing is a critical security measure that strictly isolates JULES's activities, preventing them from affecting the broader Google Cloud infrastructure, other users' VMs, or the developer's own systems. This containment is essential for safely executing potentially complex code modifications and build processes.  
The provision of a "full Linux environment" 2 implies the availability of a vast array of common command-line utilities (e.g., git for repository operations, bash for scripting, file manipulation tools like grep, sed, awk, and potentially standard compilers like gcc or interpreters like python or node, if included in a base VM image or made easily installable via setup scripts). Even if a comprehensive list of pre-installed software is not publicly itemized (an acknowledged information gap 12), this Linux foundation grants JULES significant latent capabilities. It can interact with files, manage system processes, and execute diverse build and test sequences as defined by user-provided setup scripts, offering considerable flexibility.  
However, while the environment is confirmed as "full Linux," the precise specifications of the VM resources—such as the number of CPU cores, amount of RAM, available disk space, and network bandwidth—are not publicly detailed in the available documentation. There is a mention of a token limit for the underlying language model ("Large files (over 768,000 tokens) may exceed its processing capacity" 10), which pertains to the AI's processing of textual data. However, separate VM resource constraints could also become a factor when handling very large repositories or computationally intensive build and test processes. Users should be aware that these VM resources are managed by Google and are likely not user-configurable. This may impact performance on extremely demanding projects, and it underscores the importance of structuring projects and setup scripts efficiently to work optimally within the provided environment. The specifics of the VM's technical configuration remain proprietary, but the focus is on providing a capable and secure execution context.

#### **2.2.2 Resource Allocation and Internet Access**

The JULES VM is equipped with **internet access**.2 This connectivity is crucial for modern software development, as it allows JULES to fetch dependencies from public repositories (e.g., packages from npm, PyPI, Maven Central), download necessary tools or compilers not present in the base VM image (if permitted by setup scripts), or even access online documentation if required to understand certain APIs or technologies relevant to the assigned task.  
As previously noted, specific details regarding the allocation of CPU, RAM, and disk space for each JULES VM are not publicly available. These resources are managed by the JULES platform.  
Google's decision to keep the exact VM specifications opaque is a common practice for managed cloud services. This level of abstraction allows Google to optimize, update, or modify the underlying infrastructure seamlessly, without altering the service contract or user experience. For instance, Google might upgrade the VM types or adjust resource allocations based on observed usage patterns or to incorporate newer, more efficient hardware, all without requiring users to make any changes to their JULES workflows. However, this also means that users cannot directly tune VM performance parameters (like selecting a VM with more RAM or CPU). They must rely on efficient project setup, well-structured code, and clear task definitions to ensure optimal performance of JULES on their repositories. The lack of granular detail on VM specifications is confirmed by available analyses.12  
Internet access is an indispensable capability for JULES, primarily enabling it to fetch the external libraries and packages that most modern software projects rely on. However, this connectivity also introduces considerations related to security and the reproducibility of builds. While the VM itself is sandboxed, JULES will execute commands (often defined in user-provided setup scripts) that can download and run external code. A formal FAQ document explicitly advises users: "Be cautious with third-party packages or shell commands that could compromise your system... You are responsible for the code you run".12 This highlights a shared responsibility model: Google provides a secure, sandboxed environment, but the user is responsible for the safety and integrity of the scripts and dependencies their project instructs JULES to use. Reproducibility can also be a concern if external dependencies are not version-pinned and change over time; however, standard development practices like using lock files (e.g., package-lock.json, poetry.lock, Gemfile.lock) can mitigate this risk, and JULES would respect these files if they are present and utilized by the project's setup scripts.

### **2.3 The Execution Lifecycle: From Prompt to Pull Request**

The process JULES follows from receiving a developer's request to delivering a proposed solution is carefully orchestrated to ensure transparency, control, and quality.

#### **2.3.1 Multi-Step Planning and Transparency**

A cornerstone of JULES's methodology is its **generation of a structured, multi-step plan** before it undertakes any code modifications.1 This plan is not a mere formality; it is a detailed articulation of the sequence of actions JULES intends to perform to address the given prompt or task. It serves as a roadmap for the AI's work.  
The plan is typically described in **natural language, often detailing specific sub-tasks, assumptions JULES is making based on its understanding of the request and the codebase, and the rationale behind its proposed approach**.1 For example, if asked to upgrade a Node.js version, the plan might include steps like "Update version in package.json," "Run tests to verify," "Fix any compatibility issues," and "Commit changes," potentially with further details like "will use nvm to install Node 18 and update engines field".2  
Crucially, this comprehensive plan is **presented to the developer for thorough review and requires explicit approval *before* JULES proceeds with any code execution or modification**.1 JULES is designed to make its reasoning transparent by clearly explaining "what it intends to do and why".2 This upfront clarity allows the developer to understand the proposed changes, anticipate potential impacts, and ensure the AI's interpretation aligns with their goals.  
The explicit planning phase serves as a practical and effective form of Explainable AI (XAI) specifically tailored for coding agents. By articulating its intended steps and the underlying reasoning 2, JULES offers valuable insight into its decision-making process. While this does not equate to a full deep-dive into the Large Language Model's internal state, this level of transparency provides developers with a comprehensible understanding of the agent's proposed solution. This, in turn, fosters trust and allows for informed approval, rejection, or requests for modification of the plan, aligning perfectly with the "developer in the loop" philosophy.  
Furthermore, the ability for developers to not only review the plan but also to "modify or refine the plan via an interactive chat if something looks off" 2 or to "chat with Jules to refine its plan or code" 2 transforms the interaction from a simple command-response cycle into a more collaborative dialogue. This iterative feedback loop is vital for tackling complex or nuanced tasks where the initial prompt might not have captured every subtlety or project-specific constraint. It allows the developer to effectively steer JULES towards the optimal solution, much like guiding a human junior developer by providing clarifications or course corrections during the planning or even execution phase.2

#### **2.3.2 Developer Review and Approval Workflow**

The developer's involvement is integral to the JULES workflow. As highlighted, the developer must **explicitly approve JULES's proposed plan** before any code generation or modification begins.1 This approval step acts as a critical gate, ensuring that the AI does not proceed with actions that the developer has not sanctioned.  
Following the successful execution of the approved plan within the sandboxed VM, JULES prepares its deliverables. A key output is a **visual diff (highlighting the differences) of the code changes** it has made.1 This allows for a meticulous line-by-line inspection of all additions, deletions, and modifications to the codebase. This diff view is typically available through the JULES dashboard or interface.10  
The culmination of JULES's work on a task is the **generation of a standard GitHub pull request (PR)**.1 This PR encapsulates all the changes JULES has made, along with commit messages (which may also be generated or influenced by JULES). The PR is created on a new branch in the developer's repository, ensuring that the main codebase remains untouched until the changes are formally reviewed and merged.  
This PR-centric approach seamlessly integrates JULES into established software development best practices and existing Git-based workflows.7 Developers can review JULES's contributions using the same tools and processes they use for human-authored code. They can comment on the PR, request further changes (potentially by initiating a new JULES task or making manual edits), run additional checks in their CI/CD pipeline, and ultimately decide whether to merge the AI-generated changes into their project.  
The requirement for explicit developer approval at both the planning stage and the final PR merging stage underscores the "developer in the loop" principle. JULES automates the often laborious tasks of coding, testing, and iteration, but the ultimate authority and responsibility for the codebase remain with the human developer. This balance is designed to maximize the benefits of AI assistance while maintaining rigorous quality control and developer oversight. The interactive chat feature, allowing back-and-forth communication to refine plans or correct misunderstandings even during execution 2, further empowers the developer to guide JULES effectively. If a task ultimately cannot be completed to satisfaction (e.g., tests fail persistently and JULES cannot resolve the issues), JULES will mark the task as failed and notify the user, who can then decide to tweak the prompt, adjust the environment setup, or abandon the attempt.2

## **Part 3: Interacting with Your Codebase \- JULES and GitHub**

JULES is engineered for deep integration with the GitHub ecosystem, leveraging it as the primary platform for accessing codebases, managing changes, and collaborating with developers.

### **3.1 Seamless GitHub Integration: Connecting and Authorizing**

The initial step to utilizing JULES involves connecting it to a developer's GitHub account. This is typically achieved by visiting the JULES website (jules.google.com) and signing in with a Google account, after which the user is prompted to link their GitHub account.1  
During the GitHub connection process, developers are presented with an authentication flow where they grant JULES specific permissions. A crucial choice at this stage is whether to **grant JULES access to all repositories associated with the GitHub account or to select specific repositories** that JULES will be permitted to work on.4 This granular control allows developers to manage JULES's scope of access according to their security preferences and project needs. Once the authorization is complete, JULES typically redirects the user to its dashboard, where they can select repositories and assign tasks.10  
Developers retain control over JULES's repository access even after the initial setup. GitHub's application settings allow users to review and **modify the permissions granted to "Google Labs Jules" (or a similar identifier), adjusting repository access as needed**.12 This ensures that access rights can be updated if projects are added, removed, or if access policies change.  
The tight coupling with GitHub means that JULES operates within the developer's existing ecosystem, minimizing the need for new tools or context switching.3 It works directly with real branches and can even be triggered by GitHub Issues (a feature noted as potentially upcoming or in development 10).

### **3.2 Full Repository Context: The Key to Intelligent Changes**

A defining characteristic of JULES is its ability to operate with **full-repository context**.1 Unlike tools that might only consider a single file or a limited snippet of code, JULES clones the *entire* specified repository branch into its secure VM.1  
This comprehensive access is fundamental to JULES's advanced reasoning capabilities. By having the full codebase, including all source files, documentation, configuration, and potentially even historical commit data (though the extent of history utilization is not explicitly detailed), JULES can:

* **Understand complex interdependencies:** It can see how changes in one file might affect other parts of the system.  
* **Adhere to project-specific patterns and styles:** It can learn from the existing code to generate new code that is consistent with the project's conventions.  
* **Perform multi-file edits:** Many significant software tasks, like refactoring or implementing new features, require coordinated changes across multiple files. Full context is essential for this.6  
* **Reason about the overall architecture:** It can gain a higher-level understanding of the software's design.

This holistic understanding allows JULES to make more intelligent, contextually appropriate changes and to propose plans that are more likely to be correct and complete.1 It moves beyond simple syntactic understanding to a more semantic comprehension of the project. The power of Gemini 2.5 Pro is leveraged to process this extensive context and derive actionable insights for coding tasks.2  
The ability to "read" and modify the entire project 2 means JULES is not limited by a small context window, which can be a significant constraint for other AI coding assistants. This enables it to tackle more ambitious tasks effectively.

### **3.3 From VM to Repository: Pull Request Generation and Review**

After JULES has completed the assigned task within its isolated VM according to the developer-approved plan, it does not directly commit changes back to the main repository branch. Instead, it follows standard collaborative development practices by **creating a new branch and preparing a pull request (PR)** containing all its modifications.1  
This PR includes:

* The code changes themselves, presented as a clear diff.1  
* Commit messages, which JULES often generates to describe the changes made.1  
* A summary of the work done, sometimes including the plan it followed.

The PR is then submitted to the GitHub repository, where it appears just like any other PR created by a human team member. This allows the developer (and their team) to:

* **Review the changes thoroughly:** Examine the diff, understand the impact, and check for correctness and adherence to project standards.10  
* **Run CI/CD checks:** If the repository is configured with automated builds, tests, and linters, these will typically run against JULES's PR.  
* **Provide feedback:** Developers can comment on the PR, discuss the changes, or request adjustments.  
* **Merge with confidence:** If the changes are satisfactory, the developer can merge the PR into the target branch.10

This PR-based workflow is critical for maintaining the "developer in the loop" philosophy.2 It ensures that no AI-generated code enters the main codebase without explicit human review and approval. It also leverages the powerful collaboration and version control features inherent in GitHub. JULES acts as an autonomous contributor that proposes changes, but the final decision-making authority rests with the human developers.8 The ability to see a clean diff and the reasoning behind the changes allows reviewers to accept, tweak, or reject modifications effectively.7

## **Part 4: Available Tools and Runtimes in the JULES VM**

A key aspect of JULES's operational capability lies within the environment of its virtual machine. While an exhaustive, officially published list of every pre-installed tool and runtime within the JULES VM is not readily available 12, significant inferences can be drawn from its architecture and documented functionalities.  
The JULES VM operates as a **"full Linux environment"**.2 This designation is highly informative, as a standard Linux distribution typically includes a wide array of command-line utilities essential for software development. These would commonly encompass:

* **Version Control:** git is fundamental, given JULES's direct interaction with GitHub repositories.  
* **Shell Environment:** A bash shell or similar, enabling the execution of scripts and commands.  
* **Core Utilities:** Standard GNU core utilities for file manipulation (cp, mv, rm, mkdir), text processing (grep, sed, awk, sort, uniq), and system interaction.  
* **Networking Tools:** Utilities for making network requests (e.g., curl, wget), which are necessary for fetching dependencies if not handled by package managers.

Beyond these foundational elements, the specific software development kits (SDKs), compilers, interpreters, and runtimes available are largely determined by two factors:

1. **Base VM Image:** Google likely maintains a base VM image for JULES that may include common runtimes for its best-supported languages (see below). However, the specifics of this base image are not public.  
2. **User-Provided Setup Scripts:** This is the primary mechanism by which developers customize the JULES VM environment to suit their project's specific needs.3 JULES allows users to provide setup scripts (e.g., a shell script) that are executed within the VM after the repository is cloned but before JULES begins its main task execution. These scripts can be used to:  
   * Install specific language versions (e.g., a particular Node.js, Python, or Java version using tools like nvm, pyenv, or by downloading binaries).  
   * Install project dependencies using package managers (e.g., npm install, pip install \-r requirements.txt, mvn dependency:resolve).  
   * Set up any necessary environment variables.  
   * Compile code or run any pre-processing steps required before tests or the main task can be executed.

The FAQ explicitly states, "You can provide setup scripts to ensure your project builds and tests correctly".12 This indicates that the onus is on the developer to define the precise environment their project requires if it deviates from a very basic setup or if specific tool versions are needed.  
JULES is described as **language agnostic**, but it works best with projects that use **JavaScript/TypeScript, Python, Go, Java, and Rust**.12 This "best support" likely stems from a combination of the Gemini 2.5 Pro model's training data and potentially more refined heuristics or base environment configurations for these languages. However, its language agnosticism, coupled with the setup script capability, means that in principle, JULES could work with other compiled or interpreted languages as long as the necessary build tools, compilers, and runtimes can be installed and invoked via the setup script within the Linux VM. The success with other languages would depend on "what is installed on the VM and the clarity of the environment setup script".12  
It is important to note a current limitation: **"Long-running processes like dev servers or watch scripts aren't currently supported in setup scripts. Use discrete install/test commands instead"**.12 This implies that setup scripts should focus on finite setup and build commands, rather than initiating background services or daemons.  
The ability for JULES to invoke tools during its run, such as running shell commands, tests, or even web searches (if needed and enabled by the underlying Gemini model capabilities), is a powerful aspect of its "agent loop".2 This "real-time tool usage" allows JULES to not just generate code, but also to interact with the environment to compile, run, and verify its work, making it more than a static code generator.  
In summary, while a detailed manifest of pre-installed tools is not provided, the combination of a full Linux environment, internet access, and the crucial capability to execute custom setup scripts grants developers significant flexibility in preparing the JULES VM with the necessary tools and runtimes for their specific projects. The primary responsibility for ensuring the environment is correctly configured lies with the developer through these setup scripts.

## **Part 5: Optimizing Your Repository for JULES**

To maximize the effectiveness of JULES and ensure it can operate efficiently and accurately on a given codebase, certain repository setup and interaction practices are recommended. These range from general software engineering best practices to more JULES-specific considerations, including experimental approaches for providing persistent context.

### **5.1 General Repository Best Practices for AI Agents**

While JULES is designed to be adaptable, adhering to standard software development best practices will invariably improve its performance and the quality of its outputs. These include:

* **Clear Project Structure:** A well-organized directory structure with clear naming conventions for files and folders makes it easier for JULES (and human developers) to navigate the codebase and understand the purpose of different components.  
* **Dependency Management:**  
  * Utilize standard dependency management tools appropriate for the project's language (e.g., npm/yarn for JavaScript/TypeScript, pip/poetry/conda for Python, Maven/Gradle for Java, Cargo for Rust, Go modules for Go).  
  * Commit lock files (e.g., package-lock.json, yarn.lock, poetry.lock, Gemfile.lock) to ensure reproducible dependency installation within the JULES VM. This guarantees that JULES uses the exact same versions of dependencies that are used in development and CI environments.  
* **Comprehensive Documentation:** While JULES analyzes code, good inline code comments, well-maintained README.md files (JULES is known to scan and even suggest improvements to README.md files 1), and clear documentation for APIs and complex logic can aid its understanding of intent and functionality.  
* **Modular Code:** Well-encapsulated modules and functions with clear interfaces are generally easier for an AI to understand, modify, and test.  
* **Consistent Coding Style:** Adhering to a consistent coding style (enforced by linters and formatters if possible) reduces ambiguity and makes the codebase more predictable for JULES.  
* **Secrets Management:** Crucially, **do not commit secrets (like API keys, tokens, or credentials) directly into the repository**.12 JULES clones the entire repository, and any committed secrets would be exposed within its VM. Utilize secure methods for managing secrets, such as environment variables injected during a CI/CD process (though direct injection into the JULES VM environment by users is not explicitly detailed beyond setup script capabilities) or secure secret management services. GitHub's quickstart for securing repositories is a recommended resource.12  
* **Security Vulnerabilities:** Avoid known security vulnerabilities in dependencies or custom scripts. Regularly update dependencies and use security scanning tools.12

These practices not only benefit JULES but also contribute to overall project health, maintainability, and collaboration for human developers.

### **5.2 The JULESREADME.md: An Experimental Approach to Persistent Context**

While not an officially documented feature, the concept of a JULESREADME.md file (or a similarly named, dedicated file) presents an intriguing experimental approach for providing JULES with persistent, high-level context and project-specific instructions. This idea stems from the understanding that JULES, powered by the Gemini 2.5 Pro large language model, processes and comprehends information from the entire repository, including documentation files like README.md.1  
The rationale is that a specially crafted JULESREADME.md file, placed in the root of the repository, could serve as a "system prompt" or a "constitutional guide" specifically for JULES when it operates on that particular codebase. This file would contain information that is too general or too foundational to include in every individual task prompt but is crucial for JULES to understand the project's nuances.  
If JULES is designed or becomes capable of recognizing and prioritizing such a file, the information within could significantly influence its multi-step planning process.2 It could help JULES make more informed decisions, adhere to project-specific constraints more consistently, understand architectural patterns, or even ask more targeted clarification questions if a prompt is ambiguous in the context of these guidelines. This is analogous to how custom instructions or system prompts are used to steer general-purpose LLMs, but here it would be embedded directly within the project's context, offering a persistent layer of guidance.  
The potential for a JULESREADME.md to act as a "constitution" for the AI agent within a specific repository is compelling. LLMs can be effectively guided by pre-prompts or system messages that define their persona, operational rules, and boundaries. A JULESREADME.md could serve a similar function at the repository level. Given that JULES already scans documentation and can identify missing structures in a standard README.md 1, its ability to process a specialized JULESREADME.md is a logical extension. While its current official support for such a specific file is unconfirmed 10, experimentation by users could yield valuable insights into its efficacy.  
The effectiveness of such a JULESREADME.md would, of course, depend on Gemini 2.5 Pro's ability to consistently interpret and adhere to these natural language instructions when generating code or formulating plans. LLMs can sometimes overlook parts of a prompt or misinterpret complex instructions. Therefore, the clarity, conciseness, and actionability of the instructions within a JULESREADME.md would be paramount. Simpler, direct guidelines are more likely to be followed than overly complex or numerous rules. This remains an area ripe for user experimentation and community discovery.  
The following table suggests potential sections and content ideas for such an experimental JULESREADME.md file:  
**Table 2: Recommended JULESREADME.md Sections and Content Ideas (Experimental)**

| Section Title | Purpose/Goal | Example Content Snippet/Guideline |
| :---- | :---- | :---- |
| **Project Architecture Overview** | Provide a high-level summary of the main components, services, or layers and their interactions. | "This is a microservices application with three main services: User Service, Product Service, Order Service. They communicate via REST APIs. See docs/architecture.md for details." |
| **Key Modules & Responsibilities** | List critical modules/directories and their primary responsibilities. Highlight areas of caution. | "src/core/: Contains foundational business logic. Changes here require extensive testing. src/utils/: Common utility functions, aim for pure functions." |
| **Coding Style & Conventions** | Summarize key coding style preferences, naming conventions, or patterns that JULES should adhere to. | "All new Python functions must include type hints. Prefer list comprehensions over map/filter where readable. Max line length: 100 chars. Follow PEP 8." |
| **Important Data Structures** | Define or point to definitions of critical data structures or objects, especially if complex or nuanced. | "User object schema: { id: string, email: string, isActive: boolean, roles: string }. Do not add new fields without updating UserService.updateUserSchema()." |
| **API Endpoint Guidelines** | If JULES is to add/modify API endpoints, provide guidelines on versioning, auth, request/response formats. | "New API endpoints must be versioned (e.g., /v2/). All endpoints require JWT authentication. Responses should follow the JSend specification." |
| **Testing Philosophy & Tools** | Specify preferred testing frameworks, coverage expectations, or types of tests JULES should focus on. | "All new features must be accompanied by unit tests in Jest. Target 80% line coverage. For UI changes, also consider generating basic Storybook stories." |
| **Dependency Policies** | Rules for adding new dependencies (e.g., preferred libraries, licensing constraints). | "Avoid adding new dependencies unless absolutely necessary. Prefer libraries with MIT or Apache 2.0 licenses. Discuss any GPL dependencies before adding." |
| **JULES-Specific Directives** | Explicit instructions for JULES, e.g., how to name branches it creates, preferred commit message formats. | "When creating branches, use the format jules/\<task-descriptor\>. Commit messages should follow Conventional Commits format. Always run npm run lint and npm run test before proposing a PR." |
| **Common Pitfalls / Anti-Patterns** | Warn against common mistakes or deprecated patterns specific to this codebase. | "Avoid direct database calls from controller methods; use service layer. The OldPaymentGateway class is deprecated; use NewPaymentService instead." |
| **Preferred Libraries/Frameworks** | List preferred libraries for common tasks if multiple options exist. | "For date/time manipulation, use date-fns instead of moment.js. For state management in React components, prefer Zustand over Redux for new features." |

This proactive approach of guiding JULES through a dedicated context file, beyond the immediate task prompt, could significantly improve the quality and relevance of its suggestions and reduce the number of iterations needed to achieve the desired outcome.

### **5.3 Structuring Prompts for Maximum Effectiveness**

The quality and specificity of the prompts provided to JULES are paramount for achieving desired outcomes. Effective prompt engineering is as crucial for JULES as it is for other LLM-based systems. The more context, clarity, and detail provided in the prompt, the better JULES can understand the developer's intent and generate a relevant, accurate plan and subsequent code.  
Key principles for crafting effective JULES prompts include:

* **Be Specific and Detailed:** Vague prompts are likely to lead to misaligned or incomplete outputs.10 Instead of general requests, specify files, functions, classes, and the exact behavior desired or problem to be solved. For instance, rather than "Fix bug in calculator," a better prompt would be: "Fix the divide function in src/calculator/math.js to correctly handle division by zero by returning an error message 'Cannot divide by zero' instead of throwing an exception".10  
* **Define Scope Clearly:** Indicate the boundaries of the task. If changes should be confined to a specific module or set of files, state this explicitly.  
* **Provide Contextual Clues:** If the task involves interacting with existing code, reference specific function names, variable names, or existing patterns that JULES should follow or be aware of.  
* **State Acceptance Criteria:** If possible, describe what a successful completion of the task looks like. For example, "After implementing the feature, all existing unit tests in tests/user\_service\_test.py must still pass, and a new test test\_new\_feature\_x should cover its functionality."  
* **Break Down Complex Requests:** For very large or multifaceted tasks, consider breaking them down into smaller, more manageable sub-tasks that can be prompted to JULES sequentially. This can lead to better results and makes it easier to review JULES's plans and outputs at each stage. This also helps manage the daily task limits.3  
* **Iterate and Refine:** If JULES's initial plan or output isn't perfect, refine the prompt with more specifics or clarifications and resubmit, or use the interactive chat feature to guide it.2 One detailed account of using JULES to add a feature to a WordPress plugin highlighted the importance of precise instructions, particularly regarding UI element placement and data handling logic. The author noted, "On my first run through, I made a mistake and left out the details in square brackets... I didn't tell Jules exactly where I wanted it to place the new option. As it turns out, that omission caused a big fail. Once I added the sentence in brackets... the feature worked".16 This same account emphasized that a good understanding of the underlying codebase is necessary to instruct JULES effectively for non-trivial tasks: "if I didn't understand the underlying code, I wouldn't have instructed Jules about this, and the code would not have worked. You can't 'vibe code' something like this without knowing the underlying code".16  
* **Reference Community Resources:** There is an emerging community around JULES, and collections of "Awesome Jules Prompts" may exist or be developed, offering examples of effective prompting strategies for various tasks.4

Effective prompts for JULES can be seen as "micro-specifications." They are not merely casual requests but concise, detailed descriptions of the work to be done. The success described in the WordPress plugin example 16 was achieved by treating the prompt as a comprehensive set of requirements, covering UI details, data persistence logic, UI update behavior, and the core business rules of the new feature. This suggests that developers should adopt a mindset similar to that of a tech lead writing a detailed ticket for a junior developer when crafting prompts for substantial tasks.  
The interplay between the prompt, the existing codebase (including its documentation), and a potential experimental JULESREADME.md file would form a hierarchy of information for JULES. The prompt provides the most immediate and specific instruction for the current task. The codebase and standard documentation provide general operational context. A JULESREADME.md, if implemented and recognized, could offer overriding principles or project-specific guidance that applies across multiple tasks. JULES's ability to synthesize these varying levels of instruction effectively would be critical for its success in handling complex assignments. While the prompt is the most direct form of instruction, its interpretation can be significantly enriched and guided by the broader context JULES has access to.  
The following table outlines various prompting strategies and provides examples:  
**Table 3: JULES Prompting Strategies and Examples**

| Strategy | Example Prompt | Rationale/Why it Works |
| :---- | :---- | :---- |
| **Be Specific (File, Function, Behavior)** | "In src/services/userService.js, refactor the getUserProfile(userId) function to use async/await instead of Promises. Ensure it still fetches user data from /api/users/{userId} and handles 404 errors by returning null." | Clearly identifies the target file and function, specifies the desired change (async/await), and defines expected behavior including error handling. |
| **Define Acceptance Criteria** | "Implement a new REST API endpoint POST /api/v1/items that accepts a JSON body with name (string) and price (number). It should store the item in the 'items' PostgreSQL table and return the created item with an id. Write unit tests for this endpoint using Jest, ensuring validation for missing fields and correct database insertion." | Sets clear expectations for functionality, data handling, and testing, allowing JULES to work towards a verifiable outcome. |
| **Provide Negative Constraints** | "Update the dependency lodash to the latest version across the project. Do *not* update react or react-dom as we are planning a separate migration for those." | Helps JULES avoid unintended changes by explicitly stating what it should not do, narrowing its scope of action. |
| **Reference Existing Patterns** | "Add a new configuration option enableCaching to the ConfigService in src/config/config.service.ts. Follow the existing pattern for adding boolean options, including validation and default value handling as seen with the enableLogging option." | Guides JULES to maintain consistency with the existing codebase by pointing to a relevant example to emulate. |
| **Break Down Complex Requests** | *Task 1:* "Define the database schema for a 'products' table with columns: id (PK, UUID), name (VARCHAR(255)), description (TEXT), price (DECIMAL(10,2)), created\_at (TIMESTAMP), updated\_at (TIMESTAMP). Generate the SQL migration script." *Task 2:* "Create a ProductRepository class in src/repositories/ with CRUD methods for the 'products' table using TypeORM." | Decomposes a larger feature into smaller, more manageable steps, making it easier for JULES to process and for the developer to review. |
| **Specify Output Format/Structure** | "Generate a Markdown document summarizing all public functions in the src/api/ directory. For each function, list its name, parameters (with types), return type, and a brief one-sentence description of its purpose. Group functions by file." | Provides clear instructions on the desired structure and content of the output, especially useful for documentation or report generation tasks. |
| **Ask for Alternatives (Experimental)** | "Propose two different implementations for a caching mechanism for the getProductDetails(productId) function: one using an in-memory LRU cache, and another using Redis. Provide pros and cons for each in the context of our current Node.js/Express stack." | Leverages JULES's ability to generate code to explore different solutions, potentially aiding in design decisions. Requires careful review and understanding of the implications. |

## **Part 6: Automated Testing with JULES \- Ensuring Code Quality**

JULES is not only capable of writing and modifying application code but also possesses functionalities related to automated testing. It can generate new tests, update existing ones, and, crucially, execute test scripts provided within the repository to help validate its own changes. This capability is a significant step towards ensuring the quality and correctness of AI-generated code.

### **6.1 Integrating Test Scripts into Your JULES Workflow**

JULES is designed to work with existing testing setups in a repository. Developers can, and are encouraged to, **define test execution commands within their project's setup scripts**.3 For example, a package.json file might contain a script like "test": "jest" or "test": "mocha", which can be invoked by JULES using a command like npm run test specified in the JULES setup instructions for that repository.  
When JULES is tasked with making changes, its operational plan may include a step to **run these defined tests to verify the modifications**.2 The setup scripts are used to ensure that the project not only builds correctly but also that its tests can be executed properly within the JULES VM environment.12  
JULES itself can be prompted to **write and update unit tests**.3 This means that if a developer asks JULES to implement a new feature, they can also ask it to write the corresponding unit tests. Similarly, if JULES refactors a piece of code, it might also be able to update the affected tests.  
The ability for JULES to run existing test suites serves as an automated, first-pass quality assurance mechanism. If JULES makes a code change and the configured tests (executed via the setup script as per 12) subsequently fail, this provides immediate feedback to the JULES agent (or its underlying LLM-driven logic, as suggested by the "agent loop" concept of generating code, running it, reading results, and adjusting 2). This tight feedback loop is considerably more efficient than relying solely on human review at the pull request stage to catch regressions or errors introduced by the AI. The system is designed such that JULES "Runs unit tests to verify code changes before submitting pull requests".17  
While JULES can execute tests, it is also advised that "manual verification adds certainty".10 This suggests that while JULES's automated testing capabilities are valuable, they might not always be exhaustive or infallible, especially for complex integration scenarios or if the existing test suite lacks comprehensive coverage. Developers should view JULES's test execution as a significant quality check, but not necessarily a complete replacement for thorough human QA and broader testing strategies (e.g., end-to-end tests, performance tests, security tests) that might not be part of the script JULES runs. The precise mechanisms of how JULES interprets varied test outputs or handles complex testing scenarios are areas where more detailed public documentation might still be evolving.14

### **6.2 How JULES Utilizes Test Results**

Test results are a critical input for JULES's decision-making process after it has made code modifications. The outcome of running the test suite directly influences its subsequent actions:

* **Verification of Changes:** If the tests pass after JULES has applied its changes, this provides a degree of confidence that the modifications are correct and have not introduced regressions (at least as covered by the existing test suite).2 This positive signal would typically lead JULES to proceed with preparing a pull request.  
* **Error Detection and Iteration:** If tests fail, this signals to JULES that its proposed changes were incorrect, incomplete, or had unintended side effects. The "agent loop" concept, where JULES can generate code, run it, read the results (including test outcomes), and then adjust its approach 2, is key here. Upon encountering test failures, JULES might:  
  * Attempt to analyze the failure messages (if its underlying model, Gemini 2.5 Pro, can parse and understand them).  
  * Try to automatically fix the issues it introduced that caused the tests to fail.  
  * Re-run the tests after attempting a fix.  
* **Reporting Failure:** If JULES is unable to make the tests pass after a certain number of attempts or within certain operational constraints, it will typically mark the task as failed and notify the user.2 The user can then review the situation, potentially refine the prompt, adjust the setup scripts or codebase, and ask JULES to retry the task.

The ability of JULES to not only write tests but also to then write the application code to make those tests pass opens up the potential for a form of AI-driven Test-Driven Development (TDD). A developer could theoretically prompt JULES: "Write unit tests for a function X that performs Y, ensuring edge cases A and B are covered. Then, implement function X to make all these tests pass." JULES could generate the tests first, observe them failing (as the function X wouldn't exist or be correctly implemented yet), then generate the code for function X, and iteratively re-run the tests until they all pass. This represents a more advanced workflow but is plausible given its documented capabilities for both test generation and code implementation based on test outcomes.  
The richness and clarity of the test output provided by the testing framework can significantly impact JULES's ability to debug its own changes. If tests only provide a binary pass/fail status, JULES has limited information to work with. However, if the test runner produces detailed, parseable error messages, stack traces, and context about the failures, the Gemini 2.5 Pro model might be able to use this richer information to pinpoint the root cause of the errors more effectively and propose more accurate fixes. This suggests that well-structured, informative test reporting is beneficial not just for human developers but also for maximizing JULES's effectiveness.

### **6.3 Best Practices for Writing JULES-Friendly Tests**

To ensure that JULES can effectively utilize a project's automated test suite, tests should be written and structured in a way that is amenable to AI interaction. While specific guidelines for "JULES-friendly tests" are not explicitly documented, general testing best practices, combined with an understanding of how an AI agent might interact with them, suggest the following:

* **Runnable via Discrete Commands:** Tests must be executable through clear, discrete commands that can be included in the JULES setup script (e.g., npm test, pytest, go test./...).12 The setup script should handle any necessary environment setup or compilation steps before the test command is run.  
* **Clear Pass/Fail Exit Codes:** Test runners should reliably exit with a zero status code for success (all tests passing) and a non-zero status code for failure. This is the primary signal JULES uses to determine if its changes were successful.  
* **Atomicity and Independence:** Each test case should be independent and self-contained. Tests should not rely on the state or side effects left by previously executed tests. This is important because JULES or its test runner might execute tests in a different order or context than a human developer might.  
* **Speed and Efficiency:** Faster tests lead to faster feedback loops for JULES. If tests are very slow, it will increase the overall time JULES takes to complete a task, especially if it needs to iterate based on test failures. Optimize test execution time where possible.  
* **Deterministic Results:** Tests must be deterministic, meaning they produce the same outcome (pass or fail) every time they are run with the same code. Flaky tests (tests that sometimes pass and sometimes fail without code changes) will provide confusing and unreliable signals to JULES, hindering its ability to validate its work.  
* **Descriptive Naming and Output:**  
  * Use clear, descriptive names for test suites and individual test cases. This helps JULES (and human reviewers of its PRs) understand the purpose of each test and the nature of any failures.  
  * Ensure that test failure messages are informative, providing enough context for JULES (or a human) to understand what went wrong, what was expected, and what was actually received. Well-structured error messages are more easily parsed and understood by an LLM.  
* **Good Coverage:** While JULES doesn't directly measure test coverage (unless a tool for it is invoked via script), a test suite with comprehensive coverage of the codebase provides more confidence that JULES's changes are correct. The more thoroughly the existing behavior is specified by tests, the better JULES can be guided.  
* **Avoid External Dependencies Where Possible (in Unit Tests):** Unit tests, in particular, should focus on testing individual units of code in isolation, mocking external dependencies like databases, network services, or file systems. This makes tests faster, more reliable, and easier for JULES to reason about. Integration tests will handle interactions with real services.

Well-written tests act as an executable specification of correct behavior. When JULES is tasked to refactor code or add a new feature, the existing test suite effectively defines the invariants that must be maintained or the new behaviors that must be achieved. This provides a more formal and less ambiguous form of guidance than natural language prompts alone.  
A sophisticated JULES might even attempt to co-evolve code and tests. If JULES modifies application code in such a way that existing tests legitimately need to be updated (e.g., a function signature changes as part of a refactoring, or a feature's behavior is intentionally altered), it might also propose changes to the corresponding test files. Its documented ability to "write and update unit tests" 6 supports this possibility. This would be a complex behavior requiring careful review, but it reflects a deeper understanding of the relationship between code and its tests.

### **6.4 Interpreting JULES's Actions Based on Test Outcomes**

Understanding how JULES reacts to test failures, and how developers should interpret these situations, is important for effective collaboration:

* **Successful Test Pass:** If JULES reports that all tests passed after its modifications, it will typically proceed to create a pull request. Developers should still review the changes and the scope of the tests to ensure comprehensive validation.  
* **Test Failures and JULES's Response:**  
  * If JULES encounters test failures, it may attempt to fix the issues it introduced. Its ability to do so will depend on the complexity of the failure and the clarity of the test output.  
  * If JULES successfully fixes the issue and tests then pass, this iterative process will be part of its work before generating a PR. The final PR should ideally reflect the state where all tests pass.  
  * If JULES cannot resolve the test failures, it will mark the task as failed and notify the user.2 The notification might include information about the failures encountered.  
* **Developer Action on JULES Task Failure:** When a JULES task fails due to persistent test failures:  
  * **Review the Logs/Output:** Examine any logs or error messages provided by JULES or the test runner to understand the nature of the failures.  
  * **Analyze JULES's Attempted Changes:** If possible, inspect the changes JULES was trying to make that led to the failures.  
  * **Refine the Prompt:** The original prompt might have been ambiguous or lacked crucial context, leading JULES down an incorrect path. Refining the prompt with more specific instructions or constraints can help.  
  * **Check Repository Setup:** Ensure that the setup scripts are correct, all necessary dependencies are installed in the VM, and the test environment is properly configured. An issue in the environment setup can lead to spurious test failures.  
  * **Examine the Codebase/Tests:** There might be underlying issues in the codebase or flakiness in the tests themselves that confused JULES or made it difficult for its changes to pass. One user's experience (albeit with a different agent, then compared to JULES) described a scenario where AI-generated changes led to catastrophic test failures due to syntax errors, highlighting the importance of robust testing and careful review of AI contributions.19  
  * **Rerun the Task:** After making adjustments, the task can be rerun.

The scenario where JULES introduces a change that breaks tests, and its own attempts to fix the problem also fail, can be thought of as a "debugging the debugger" situation. This is where the "developer in the loop" 2 becomes absolutely crucial. The human developer needs to diagnose why JULES failed and provide better guidance, whether by improving the prompt, enhancing the setup script, or addressing underlying issues in the codebase or test suite that may have misled the AI.  
Consistent patterns of test failures caused by JULES's modifications can also serve as valuable learning opportunities. For the user, it might indicate areas where their prompts need to be more explicit or where their repository setup might be suboptimal for AI interaction. For Google's JULES development team, such patterns could highlight areas where the underlying Gemini model's reasoning or code generation capabilities need further refinement or fine-tuning.

## **Part 7: Experimental Ideas and The Future of JULES**

JULES, with its full repository context, advanced reasoning powered by Gemini 2.5 Pro, sandboxed VM execution environment, support for setup scripts, concurrent task handling 2, and real-time tool use capabilities 2, provides a robust foundation for exploring applications beyond routine bug fixes and dependency updates. This section delves into potential untested and novel use cases, encouraging user experimentation, and discusses the evolving landscape of this agentic coding assistant.

### **7.1 Pushing the Boundaries: Untested and Novel Use Cases**

The true potential of an AI agent like JULES is often unlocked through creative application and experimentation. Users are encouraged to explore the following, and other, more ambitious use cases, keeping in mind the current beta status and usage limits:

* **Automated Large-Scale Refactoring:**  
  * **Concept:** Prompt JULES to undertake complex refactoring initiatives that span multiple files, modules, or even architectural layers.  
  * **Example Prompt:** "Refactor all classes within the com.example.legacy.util package to replace the custom LegacyStringHelper class with equivalent functionalities from org.apache.commons.lang3.StringUtils. Ensure all existing unit tests in the com.example.legacy.test package continue to pass after the refactoring. Update all import statements accordingly."  
  * **Leveraged Capabilities:** Full-repository context, multi-file editing, ability to run tests for verification, code understanding for semantic equivalence.  
* **JULES-Driven API Client Generation or SDK Updates:**  
  * **Concept:** If a project exposes an API defined by an OpenAPI/Swagger specification (or a similar machine-readable format), task JULES with generating or updating client libraries in various programming languages.  
  * **Example Prompt:** "Given the openapi.yaml in the project root, generate a basic Python client library for all GET and POST endpoints. Include simple request/response handling and basic error management. Place the generated client in a new clients/python/ directory and write a minimal test\_client.py that attempts to call one GET endpoint (mocking the server response)."  
  * **Leveraged Capabilities:** Code generation in multiple languages, file system manipulation, test generation, understanding of API specifications.  
* **Attempted Security Vulnerability Remediation (with Extreme Caution and Thorough Review):**  
  * **Concept:** Provide JULES with a security vulnerability report (e.g., from a static analysis security testing (SAST) tool or a dependency scanner) and ask it to attempt to patch the identified issues.  
  * **Example Prompt:** "The attached sast\_report.json identifies a potential SQL injection vulnerability in src/data/queryBuilder.java at line 75\. Attempt to refactor the buildQuery method to use parameterized queries or a safe query construction mechanism to mitigate this vulnerability. Verify by re-running the specific test case TestSQLInjectionScenarios.java."  
  * **Leveraged Capabilities:** Bug fixing, code understanding (security context), ability to run specific tests. **Note:** This is a high-risk use case. All changes proposed by JULES for security vulnerabilities *must* be meticulously reviewed by security experts.  
* **Prototyping Entire Features from High-Level Specifications:**  
  * **Concept:** Challenge JULES to build out the scaffolding or even a functional first version of a new feature based on a relatively high-level description.  
  * **Example Prompt:** "Implement a new feature: a user notification system.  
    1. Define a new database table schema for 'notifications' (columns: id (PK), user\_id (FK), message (TEXT), is\_read (BOOLEAN, default FALSE), created\_at (TIMESTAMP)). Generate the necessary database migration.  
    2. Create a new API endpoint POST /users/{userId}/notifications to send a notification to a user.  
    3. Create an API endpoint GET /users/{userId}/notifications to retrieve unread notifications for a user.  
    4. Write unit tests for the backend logic of these new endpoints."  
  * **Leveraged Capabilities:** Feature building 1, test writing 3, database schema understanding (if supported by model), API design.  
* **Automated Documentation Generation and Updates:**  
  * **Concept:** Task JULES with scanning the codebase and generating or updating inline documentation (e.g., JSDoc, TSDoc, JavaDoc, Python docstrings) or even external Markdown documentation.  
  * **Example Prompt:** "Scan all Python files in the src/core/ directory. For every public function and class method that lacks a docstring, generate a concise docstring explaining its purpose, parameters (with types), and return value (with type). If docstrings exist but function signatures have changed, attempt to update the docstring parameters to match."  
  * **Leveraged Capabilities:** Code understanding, documentation generation 1, understanding of code structure and signatures.  
* **Cross-Repository Task Coordination (Highly Experimental and Likely Beyond Current Public Beta):**  
  * **Concept:** If future iterations of JULES allow for authorized access and coordination across multiple repositories owned by the same user/organization, one could envision prompting it for changes that span microservices or related projects.  
  * **Example Prompt (Hypothetical):** "In user-service-repo, update the User API schema (version 2.0) to include a new lastLoginIp field. Then, in order-service-repo, update its client code to correctly consume the User API v2.0, ensuring it can handle the new lastLoginIp field. Ensure all tests pass in both repositories."  
  * **Leveraged Capabilities (Hypothetical):** Concurrent task handling across different contexts 2, complex planning, inter-service dependency understanding. This is speculative but aligns with the long-term vision of highly capable AI agents.

These experimental use cases push JULES beyond being a simple assistant towards becoming a more proactive "teammate" 4 capable of taking on significant, somewhat open-ended development tasks. The success of such experiments will heavily depend on the developer's ability to clearly define high-level goals, constraints, and acceptance criteria, effectively acting as a "product owner" or "technical lead" for the AI agent's work. Developers could also use JULES as an "automated research assistant" for code, prompting it to explore different implementations of an algorithm or to quickly prototype alternative solutions to a problem, potentially including benchmarks if JULES can execute them in its VM.2

### **7.2 Encouraging User Experimentation: Tips for Discovering New Capabilities**

The full spectrum of JULES's capabilities and limitations will likely be uncovered by the user community through diverse and creative experimentation. To foster this discovery process, consider the following tips:

* **Start Small and Iterate:** Begin with well-defined, relatively simple tasks to understand JULES's baseline behavior, its planning style, and the quality of its output for your specific codebase and language.4  
* **Incrementally Increase Complexity:** Once comfortable with simpler tasks, gradually increase the complexity of your prompts. If a very complex prompt fails or yields unsatisfactory results, try breaking it down into smaller, sequential sub-prompts, as demonstrated by the successful WordPress plugin feature addition.16  
* **Analyze JULES's Plans Carefully:** The multi-step plan JULES proposes before execution is a window into its interpretation of your prompt and its intended approach. Reviewing this plan thoroughly can help you identify misunderstandings early and refine your prompt or the plan itself.  
* **Utilize Interactive Refinement:** If the JULES interface supports interactive chat or plan modification 2, use this capability to steer JULES if its initial plan is not quite right or if it encounters issues during execution.  
* **Experiment with the JULESREADME.md Concept:** Try creating a JULESREADME.md file as described in Part 5.2. Populate it with project-specific guidelines and observe if it influences JULES's behavior or the quality of its plans and code. Share your findings and effective patterns with the broader developer community.  
* **Systematically Test Prompting Styles:** For a given type of task (e.g., refactoring, test generation), try different prompting styles. Vary the level of detail, the way constraints are phrased, and the amount of contextual information provided to see what yields the best results for your specific needs.  
* **Identify Repetitive or Tedious Tasks:** Think about the coding tasks in your daily workflow that are time-consuming, repetitive, or generally considered "grunt work." These are often excellent candidates for JULES to automate or assist with.2  
* **Be Mindful of Usage Limits:** During the public beta (and potentially beyond), JULES has daily task limits and concurrent task limits.3 Plan your more extensive experiments accordingly to avoid hitting these quotas prematurely.  
* **Share Your Discoveries:** As you find novel ways to use JULES, effective prompting techniques, or limitations, consider sharing your experiences through blog posts, forums, or community channels. The collective knowledge of the user base will be invaluable in shaping the understanding and future development of JULES. The mention of a collection of "Awesome Jules Prompts" 4 suggests this community-driven discovery process is already underway.

### **7.3 The Evolving Landscape: What to Expect Next**

JULES is not a static product; it is an actively developing platform currently in public beta.4 As such, its capabilities are expected to evolve and expand over time. Several indicators and logical extensions point towards future directions:

* **Enhanced Testing and Integration Capabilities:** Deeper support for various types of testing (beyond unit tests), potential for UI previews of changes, and richer integrations with CI/CD pipelines are anticipated as JULES matures.1 This would allow JULES to participate more fully in the end-to-end development and deployment lifecycle.  
* **Enterprise Features and Flexible Pricing:** Additional pricing tiers beyond the initial free beta offering, along with enterprise-focused management features, are expected to be introduced later in the year.11 This will likely include options for increased usage quotas, team management, and potentially more advanced security and compliance controls.  
* **Advancements in Agentic AI:** Google is positioning JULES as a key player in the future of "agentic development".6 This suggests a strategic commitment to evolving JULES into an even more autonomous and capable AI partner. The development of "Agentic Colab" 17 and other agentic AI initiatives within Google indicate a broader ecosystem of intelligent agents is being cultivated.  
* **Improvements from Underlying Model Evolution:** As the core Gemini models (Gemini 2.5 Pro and future iterations) become more powerful—gaining longer context windows, more sophisticated reasoning abilities, enhanced multimodal understanding, or better proficiency in new programming languages and paradigms—JULES will directly benefit from these advancements.  
* **Broader Tool and Platform Integration:** While currently focused on GitHub, integrations with other version control systems, IDEs (beyond the existing Gemini integration in Android Studio 8), project management tools, and other developer platforms are logical future steps to broaden JULES's applicability.  
* **Increased Task Complexity Handling:** The "autonomous developer intern" 1 may metaphorically "grow up." As JULES matures, it is expected to take on increasingly complex and responsible tasks, potentially moving from handling primarily routine chores to assisting with more sophisticated design and implementation challenges. Current limitations, such as those on long-running processes in setup scripts 12 or the continued need for thorough manual verification 10, may be addressed or mitigated over time.

The development of JULES will likely be influenced by broader trends in AI research, including advancements in large language models, agent architectures, and MLOps for AI-driven software development. The vision of "agentic development" 6 implies a trajectory towards more capable, independent, and collaborative AI agents that become indispensable members of software engineering teams.

## **Part 8: Appendix**

### **8.1 JULES Usage Limits and Quotas (Public Beta)**

During its public beta phase, JULES is available free of charge but is subject to certain usage limits. Developers should be aware of these quotas to plan their usage effectively:

* **Daily Task Limit:** Most sources consistently indicate that users are provided with **five (5) free tasks per user per day**.3  
  * *Note:* Some sources 4 have mentioned a higher limit of 60 tasks per day. However, the preponderance of evidence from multiple announcements and guides points to the 5 tasks/day limit during the current public beta. Users should refer to the official JULES dashboard or documentation for the most current information.  
* **Concurrent Task Limit:**  
  * Several sources indicate a maximum of **five (5) concurrent tasks**.4  
  * One source 3 mentions a maximum of **three (3) concurrent tasks**.  
  * This discrepancy suggests that the concurrent task limit might vary or has been adjusted. Again, the official JULES interface is the best source for the current effective limit.  
* **Audio Changelog ("Codecast") Limit:** Users can generate a maximum of **five (5) audio changelogs (codecasts) per day**.3

If these limits are exceeded, the ability to initiate new tasks will be temporarily disabled. Existing tasks can still be managed, and task history is preserved.3 These limits are in place to manage resources during the beta period and may be subject to change as JULES evolves and introduces potential pricing tiers.11

### **8.2 Troubleshooting Common Issues**

As with any advanced software, particularly one in public beta, users may occasionally encounter issues. Here are some common problems and suggested troubleshooting steps, based on user experiences and general best practices 10:

* **Task Fails to Start:**  
  * **Check Internet Connection:** Ensure your local machine (if initiating from a web interface) and the JULES service have stable internet connectivity.  
  * **Verify GitHub Permissions:** Confirm that JULES still has the necessary permissions to access the selected repository on GitHub. Permissions might have changed, or the token might have expired or been revoked. Check GitHub settings under Applications \> Authorized OAuth Apps for "Google Labs Jules".12  
* **Unexpected or Misaligned Changes from JULES:**  
  * **Refine Your Prompt:** If JULES produces code or a plan that doesn't match your intent, the prompt was likely too vague or lacked sufficient detail. Add more specifics about files, functions, desired behavior, and constraints, then resubmit the task.10  
  * **Break Down the Task:** For complex requests, divide the task into smaller, more focused sub-tasks.  
* **Pull Request (PR) Not Created:**  
  * **Check Repository Write Access:** Ensure that JULES (via the GitHub App installation) has write permissions to the repository. Without write access, it cannot create new branches or push PRs.  
  * **Review Task Logs (if available):** Look for any error messages from JULES that might indicate why PR creation failed.  
* **Slow Performance or Task Timeouts:**  
  * **Large Codebases:** Very large repositories may take longer for JULES to clone, scan, and process. This can lead to slower task completion times.10  
  * **Complex Setup Scripts:** Lengthy or computationally intensive setup scripts will add to the overall task duration. Optimize setup scripts for efficiency.  
  * **Split Complex Tasks:** If a single task is extremely complex (e.g., refactoring thousands of files), consider splitting it into smaller, more manageable units.  
* **Beta Quirks and Occasional Downtime:**  
  * As a beta product, JULES may experience occasional bugs, unexpected behavior, or temporary service disruptions.10 Check official Google Cloud status pages or JULES community channels for any known issues.  
* **Context Constraints (LLM Token Limits):**  
  * The underlying Large Language Model (Gemini 2.5 Pro) has a maximum context window or token limit. Very large individual files (e.g., exceeding 768,000 tokens as mentioned in one source 10, though this specific number can vary by model version) may be difficult for JULES to process in their entirety for certain types of analysis or modification. This is less about the total repository size and more about the size of individual artifacts the LLM needs to "read" at once.  
* **Setup Script Failures:**  
  * Ensure your setup scripts are robust, handle potential errors gracefully, and use discrete install/test commands rather than long-running processes.12 Test your setup scripts thoroughly in a clean Linux environment similar to what JULES might use.

### **8.3 Security and Privacy Considerations (Reiteration and Emphasis)**

Security and data privacy are paramount when integrating an AI agent like JULES into software development workflows. Google has implemented several measures and provides clear guidelines in this regard:

* **Private Code Stays Private:**  
  * JULES runs tasks privately by default when operating on private GitHub repositories.11  
  * Crucially, **Google states that it does not use code from users' private repositories to train its general JULES AI models**.3 This is a critical assurance for protecting intellectual property.  
* **Opt-Out for Public Repositories:** For public repositories, users typically have an option in the JULES settings to **disallow their content from being used for AI model training**.2 It is recommended to review and configure this setting according to preference.  
* **Isolated and Temporary Execution Environment:**  
  * Each JULES task runs in a **fresh, sandboxed, and isolated Google Cloud virtual machine**.2  
  * All data related to a task (cloned code, installed dependencies, intermediate files) is **temporary and securely deleted after the task is completed**.3  
* **User Responsibility for Code and Dependencies:**  
  * Developers are reminded that **they are responsible for the code they ultimately run and merge into their projects**, including code generated or modified by JULES.12 Thorough review of all PRs from JULES is essential.  
  * **Do not commit secrets** (API keys, passwords, tokens, credentials) to your repository. JULES clones the entire repo, and any such secrets would be exposed within its temporary VM.12  
  * **Be cautious with third-party packages, dependencies, and shell commands** included in setup scripts or invoked by the project. Ensure they are from trusted sources and do not contain known security vulnerabilities.12  
* **Limited Scope of Access:** JULES operates strictly within the authorized GitHub scope granted by the user during the account linking process.3 Permissions can be reviewed and adjusted in GitHub settings.12

By understanding and adhering to these security and privacy principles, developers can leverage JULES's powerful capabilities while maintaining control over their codebase and sensitive information.

### **8.4 JULES in the AI Coding Assistant Landscape: A Comparative Overview**

To better understand JULES's unique positioning, it's helpful to compare it with other prominent AI coding assistants like GitHub Copilot and OpenAI Codex (which powers many code generation features).  
**Table 4: JULES vs. GitHub Copilot vs. OpenAI Codex \- Comparative Overview**

| Feature | JULES | GitHub Copilot | OpenAI Codex (underlying model) |
| :---- | :---- | :---- | :---- |
| **Operational Mode** | **Asynchronous Agent**.2 Works in the background on delegated tasks. | **Synchronous, Inline Assistant**.1 Provides real-time code suggestions and completions within the IDE. | Powers various applications, can be synchronous or asynchronous depending on implementation. |
| **Primary Use Case** | End-to-end task automation (bug fixing, refactoring, feature scaffolding, dependency upgrades).1 | Code completion, boilerplate generation, inline function generation, learning/exploration. | Code generation, translation, explanation; foundational model for tools like Copilot and others. |
| **Context Window** | **Full Repository Context**.1 Clones entire repo into VM. | Primarily current file and open tabs, with some broader project context awareness. | Varies by specific model version and implementation; often limited to prompt and recent conversation. |
| **Planning Transparency** | **High:** Generates a multi-step plan for review and approval before execution.2 | **Low to Moderate:** Suggests code directly; planning is implicit in its suggestions. | N/A (model itself); planning depends on how it's prompted and used by an application. |
| **Execution Environment** | **Secure Google Cloud VM**.2 Sandboxed Linux environment with internet access. | Runs within the developer's IDE; execution of suggested code is manual by the developer. | Execution environment is determined by the application using the Codex API. |
| **Output Format** | GitHub Pull Requests with diffs, commit messages, audio changelogs.1 | Inline code suggestions, chat-based code blocks. | Raw code, text explanations, or other formats depending on the prompt. |
| **Key Strength** | Autonomous execution of complex, multi-file tasks; structured planning; deep repo understanding.1 | Rapid inline code generation; reducing boilerplate; "pair programmer" feel for active coding. | Powerful general-purpose code understanding and generation; flexibility for diverse applications. |
| **Developer Interaction** | Task delegation, plan review/approval, PR review, interactive chat for refinement.2 | Prompting via comments or code context, accepting/rejecting suggestions, chat interaction. | Prompt engineering; interaction model defined by the consuming application. |
| **Underlying Model (Example)** | Google Gemini 2.5 Pro.2 | OpenAI Codex-derived models. | GPT-3 / GPT-4 series (Codex models are specialized versions). |

This comparison highlights that JULES is designed to operate more like an "autonomous developer intern" 1, capable of taking on and independently executing complete coding tasks with oversight, whereas tools like GitHub Copilot are more akin to highly responsive "pair programmers" offering real-time assistance during active coding. The choice between them depends on the nature of the task and the desired level of AI autonomy.

## **Conclusions**

Google JULES emerges as a sophisticated and powerful asynchronous AI coding agent, poised to significantly impact developer workflows and productivity. Its architecture, centered around the advanced reasoning capabilities of the Gemini 2.5 Pro model and a secure, full-context virtual machine environment, enables it to tackle complex, multi-file software engineering tasks with a notable degree of autonomy. The "developer in the loop" philosophy, maintained through transparent multi-step planning, explicit approval stages, and familiar pull request-based integration with GitHub, is crucial for building trust and ensuring that developers retain ultimate control over their codebases.  
The ability of JULES to not only modify code but also to write, update, and execute automated tests represents a significant step towards AI-assisted quality assurance. While still in public beta and subject to evolution, the potential for JULES to handle routine maintenance, accelerate feature development, and improve code quality is substantial.  
Optimal interaction with JULES requires clear, specific prompting and well-structured repositories. The experimental concept of a JULESREADME.md file offers a promising avenue for providing persistent, project-specific guidance to the agent, potentially enhancing its performance and alignment with project conventions.  
As JULES matures, deeper integrations with testing frameworks, CI/CD pipelines, and broader enterprise features are anticipated. Its development signifies a clear move towards more "agentic" AI in software engineering, where AI partners take on increasingly complex responsibilities, allowing human developers to focus on higher-level design, innovation, and problem-solving. Users are encouraged to experiment with JULES, push its boundaries, and contribute to the collective understanding of how such advanced AI agents can best augment the art and science of software development. The journey of JULES is indicative of a future where human-AI collaboration will redefine the landscape of coding and software creation.

### **Works cited**

1. Google Jules: A Guide With 3 Practical Examples \- DataCamp, accessed June 8, 2025, [https://www.datacamp.com/tutorial/google-jules](https://www.datacamp.com/tutorial/google-jules)  
2. Jules AI SWE Agent: Google's Take on Coding Automation | Engine \- EngineLabs.ai, accessed June 8, 2025, [https://www.enginelabs.ai/blog/jules-ai-swe-agent-googles-take-on-coding-automation](https://www.enginelabs.ai/blog/jules-ai-swe-agent-googles-take-on-coding-automation)  
3. What is Jules? Google's New Asynchronous AI Coding Assistant That's Transforming Development \- GenApe, accessed June 8, 2025, [https://app.genape.ai/tutorial-article/ai-generator/jules-google-ai](https://app.genape.ai/tutorial-article/ai-generator/jules-google-ai)  
4. Google Jules: An Asynchronous Coding Agent Explained \- Habr, accessed June 8, 2025, [https://habr.com/en/articles/915534/](https://habr.com/en/articles/915534/)  
5. Building with AI: highlights for developers at Google I/O, accessed June 8, 2025, [https://blog.google/technology/developers/google-ai-developer-updates-io-2025/](https://blog.google/technology/developers/google-ai-developer-updates-io-2025/)  
6. Jules by Google: An AI agent for autonomous coding \- Cosmico, accessed June 8, 2025, [https://www.cosmico.org/jules-by-google-an-ai-agent-for-autonomous-coding/](https://www.cosmico.org/jules-by-google-an-ai-agent-for-autonomous-coding/)  
7. Getting Started with Jules, An Asynchronous AI Coding Agent by Google, accessed June 8, 2025, [https://aiagent.marktechpost.com/post/what-is-jules-an-asynchronous-ai-coding-agent-by-google](https://aiagent.marktechpost.com/post/what-is-jules-an-asynchronous-ai-coding-agent-by-google)  
8. Google crowns Jules to be its agent and spreads the AI love \- The Register, accessed June 8, 2025, [https://www.theregister.com/2025/05/21/google\_crowns\_jules\_to\_be/](https://www.theregister.com/2025/05/21/google_crowns_jules_to_be/)  
9. Google's Jules AI Coding Agent Can Assist – But Does Not Replace – Developers, accessed June 8, 2025, [https://www.techrepublic.com/article/news-google-jules-ai-agent-public-beta/](https://www.techrepublic.com/article/news-google-jules-ai-agent-public-beta/)  
10. How to Use Google Jules: A Beginners' Guide \- Apidog, accessed June 8, 2025, [https://apidog.com/blog/google-jules/](https://apidog.com/blog/google-jules/)  
11. Google Makes 'Jules' AI Coding Agent Publicly Available at I/O 2025 \- MLQ.ai | Stocks, accessed June 8, 2025, [https://mlq.ai/news/google-makes-jules-ai-coding-agent-publicly-available-at-io-2025/](https://mlq.ai/news/google-makes-jules-ai-coding-agent-publicly-available-at-io-2025/)  
12. FAQ | Jules, accessed June 8, 2025, [https://jules.google/docs/faq/](https://jules.google/docs/faq/)  
13. Google Jules 101: Master Jules AI Agent Under 10 Minutes \- YouTube, accessed June 8, 2025, [https://www.youtube.com/watch?v=EWZtE2i1tIw\&vl=id](https://www.youtube.com/watch?v=EWZtE2i1tIw&vl=id)  
14. Tried out Jules AI agent : r/programminghorror \- Reddit, accessed June 8, 2025, [https://www.reddit.com/r/programminghorror/comments/1ktlje0/tried\_out\_jules\_ai\_agent/](https://www.reddit.com/r/programminghorror/comments/1ktlje0/tried_out_jules_ai_agent/)  
15. Jules \- AI Agent Reviews, Features, Use Cases & Alternatives (2025), accessed June 8, 2025, [https://aiagentsdirectory.com/agent/jules](https://aiagentsdirectory.com/agent/jules)  
16. Google's Jules AI coding agent built a new feature I could actually ship \- while I made coffee, accessed June 8, 2025, [https://www.zdnet.com/article/googles-jules-ai-coding-agent-built-a-new-feature-i-could-actually-ship-while-i-made-coffee/](https://www.zdnet.com/article/googles-jules-ai-coding-agent-built-a-new-feature-i-could-actually-ship-while-i-made-coffee/)  
17. Google Counters GitHub & Microsoft with Jules Agent & Enhanced Gemini AI, accessed June 8, 2025, [https://visualstudiomagazine.com/articles/2025/05/20/google-counters-github-microsoft-with-jules-agent-enhanced-gemini-ai.aspx](https://visualstudiomagazine.com/articles/2025/05/20/google-counters-github-microsoft-with-jules-agent-enhanced-gemini-ai.aspx)  
18. Jules \- AI Agent Store, accessed June 8, 2025, [https://aiagentstore.ai/ai-agent/jules](https://aiagentstore.ai/ai-agent/jules)  
19. Codex, Jules, and Claude Code comparison \- Jon Atkinson, accessed June 8, 2025, [https://www.jonatkinson.co.uk/blog/codex-jules-claude-comparison](https://www.jonatkinson.co.uk/blog/codex-jules-claude-comparison)
